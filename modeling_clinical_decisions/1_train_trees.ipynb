{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-based Classifiers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# model_storage\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Model Training and Evaluation\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_predict #train_test_split,  \n",
    "from sklearn.preprocessing import MinMaxScaler #, StandardScaler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline #from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the root directory for imports\n",
    "import pyrootutils\n",
    "root = pyrootutils.setup_root(\n",
    "    search_from=os.path.abspath(''),\n",
    "    indicator=[\".git\"],\n",
    "    pythonpath=True,\n",
    "    dotenv=True,\n",
    ")\n",
    "\n",
    "from utils.file_management.config_loader import load_yaml, process_config_values\n",
    "from utils.file_management.file_manager import FileManager\n",
    "\n",
    "from utils.model_utils.data_utils import load_data, visualize_data, split_data, MultiClassToBinaryConverter\n",
    "from utils.model_utils.eval_utils import eval_report, load_json\n",
    "from utils.model_utils.shap_utils import shap_analysis_pipeline # Feature Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load yaml file with dataset information\n",
    "config_path = str(root) + '/src/config/LBP_cohort.yaml'\n",
    "config = process_config_values(load_yaml(config_path))\n",
    "# For debug\n",
    "# print(config.keys())\n",
    "\n",
    "# Load paths to data\n",
    "PlumsFiles = FileManager(config.get('file_directory'))\n",
    "\n",
    "# --- DEFINE VARIABLES ---\n",
    "# --- dataset variables ---\n",
    "# Path to preprocessed data\n",
    "master_data_path         = PlumsFiles.get_datapath('model_output_dir').replace('MODEL','classification_1class_meds').replace('INDEPENDENT_VAR','master_data_for_analysis.csv') \n",
    "master_encoded_data_path = PlumsFiles.get_datapath('model_output_dir').replace('MODEL','classification_1class_meds').replace('INDEPENDENT_VAR','master_numerical_data_for_analysis.csv') \n",
    "save_path_reference = PlumsFiles.get_datapath('model_output_dir')\n",
    "\n",
    "# Data options\n",
    "time_frame_list = ['2012_to_2024', '2012_to_2018', '2019_to_2024', '2012_to_2016', '2017_to_2019', '2020_to_2024'] # Options (1 set, 2 sets, 3 sets): ['2012_to_2024', '2012_to_2018', '2019_to_2024', '2012_to_2016', '2017_to_2019', '2020_to_2024']\n",
    "time_frame_list = ['2012_to_2024']\n",
    "data_type_list = ['tabular_text','tabular','text', 'diagnoses_tabular', 'diagnoses_text', 'psychosocial_tabular'] # TODO: Define subset of data\n",
    "\n",
    "# --- OVO and OVR variables ---\n",
    "# Define columns for creating the dataframe and data loader\n",
    "y_col = 'interventiontype'\n",
    "\n",
    "# Define label combinations for analysis\n",
    "labels_dict = {\n",
    "    0:'none',\n",
    "    1:'nsaids',\n",
    "    2: 'opioids',\n",
    "    }\n",
    "# Note: The label encoding maps label1 to 0 and label2 to 1\n",
    "paired_labels = [(0,1), (0,2), (1,2), ('rest', 0),('rest', 1),('rest', 2)] #full paired labels\n",
    "#paired_labels = list(combinations(df_data[outcome_column].unique(), 2))\n",
    "\n",
    "# --- training variables ---\n",
    "n_splits = 5\n",
    "\n",
    "model_storage = {\n",
    "    'decision_tree_best': {\n",
    "        'model': DecisionTreeClassifier(random_state=0),\n",
    "        'hyperparams': {\n",
    "            'model__criterion': ['gini', 'entropy'],\n",
    "            'model__splitter': ['best', 'random'],\n",
    "            'model__max_depth': [None, 2, 4, 6, 8],\n",
    "            'model__min_samples_split': [2, 5, 10],\n",
    "            'model__min_samples_leaf': [1, 2, 4],\n",
    "            'model__class_weight': ['balanced'],  \n",
    "        },\n",
    "    },\n",
    "    'random_forest_best': {\n",
    "        'model': RandomForestClassifier(random_state=0),\n",
    "        'hyperparams': {\n",
    "            'model__n_estimators': [50, 100, 200],\n",
    "            'model__max_depth': [10, 15, 20],\n",
    "            'model__min_samples_split': [2, 5, 10],\n",
    "            'model__min_samples_leaf': [1, 2, 4],\n",
    "            'model__bootstrap': [True, False],\n",
    "            'model__class_weight': ['balanced', 'balanced_subsample', None],  \n",
    "        },\n",
    "    },\n",
    "    'bagging_best': {\n",
    "        'model': BaggingClassifier(estimator=RandomForestClassifier(random_state=0), random_state=0),\n",
    "        'hyperparams': {\n",
    "            'model__n_estimators': [10, 50, 100],\n",
    "            'model__max_samples': [0.5, 0.7, 1.0],\n",
    "            'model__max_features': [0.5, 0.7, 1.0],\n",
    "            'model__bootstrap': [True, False],\n",
    "            'model__bootstrap_features': [True, False],\n",
    "            \n",
    "        },\n",
    "    },\n",
    "    'adaboost_best': {\n",
    "        'model': AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=0), random_state=0),\n",
    "        'hyperparams': {\n",
    "            'model__n_estimators': [50, 100, 180, 200],\n",
    "            'model__learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
    "        }\n",
    "    },\n",
    "    'XGBoost_best': {\n",
    "        'model': XGBClassifier(random_state=0),\n",
    "        'hyperparams':{\n",
    "            'model__objective': ['binary:logistic'],\n",
    "            'model__max_depth': [3, 4, 5, 6],\n",
    "            'model__learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
    "            'model__n_estimators': [50, 100, 150, 200],\n",
    "            'model__lambda': [0, 10, 50, 100],\n",
    "            'model__alpha': [0, 10, 50, 100],\n",
    "            'model__scale_pos_weight': [1, 1.4, 1.7, 3.4, 4.8, 8.2],  # Control the balance of positive and negative weights, \n",
    "        }\n",
    "    },\n",
    "    # 'LightGBM_best': {\n",
    "    #     'model': LGBMClassifier(random_state=0),\n",
    "    #     'hyperparams':{\n",
    "    #         'model__num_leaves': [31, 50],\n",
    "    #         'model__max_depth': [3, 4, 5, 6],\n",
    "    #         'model__learning_rate': [0.05, 0.1, 0.2],\n",
    "    #         'model__n_estimators': [50, 100],\n",
    "    #         'model__scale_pos_weight': [1, 1.4, 1.7, 3.4, 4.8, 8.2],\n",
    "    #         'model__reg_lambda': [0, 10, 50, 100],\n",
    "    #         'model__reg_alpha': [0, 10, 50, 100],\n",
    "    #     }\n",
    "    # },\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models\n",
    "Finetune tree-based models.<br>\n",
    "Choose the best finetuned model from architecture options based on val set metric balanced_acc_adjusted_for_chance.<br>\n",
    "Calculate performance metrics on the test set.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- Main Training Code ---\n",
    "for time_frame in time_frame_list[0]:\n",
    "    for data_type in data_type_list[0:4]:\n",
    "        analysis_path = save_path_reference.replace('MODEL','classification_1class_meds').replace('INDEPENDENT_VAR',f'v5/{data_type}/{time_frame}') \n",
    "        os.makedirs(analysis_path, exist_ok=True)\n",
    "\n",
    "        # Load, select, and visualize data\n",
    "        df_categorical, df_data, continuous =  load_data(master_data_path, master_encoded_data_path, analysis_path, data_type, time_frame)\n",
    "        visualize_data(df_categorical, analysis_path)\n",
    "\n",
    "        # Prepare data for training\n",
    "        df_dev, df_test = split_data(df_data, y_col)\n",
    "\n",
    "        categorical = [col for col in df_data.columns if col not in continuous]\n",
    "        BinaryLabelLoader = MultiClassToBinaryConverter(y_col, categorical_cols=categorical)\n",
    "\n",
    "        # Define k-fold cross-validation\n",
    "        kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "        analysis_path2 = analysis_path \n",
    "        os.makedirs(analysis_path2, exist_ok=True)\n",
    "        # Create a scorer object using balanced_accuracy_score\n",
    "        balanced_accuracy_scorer = make_scorer(balanced_accuracy_score, adjusted=True)\n",
    "\n",
    "        # --- MODEL TESTING AND HYPERPARAMETER TUNING ---\n",
    "        # Iterate over each label pair and predictor variable\n",
    "        for X_train, y_train, names in BinaryLabelLoader.process_data_stream(df_dev, paired_labels):\n",
    "            X_train = np.array(X_train)\n",
    "            y_train = np.array(y_train)\n",
    "            label_0, label_1, input_variable = names.values()\n",
    "            label_0_name = 'rest' if label_0 == 'rest' else labels_dict[label_0]\n",
    "            label_1_name = labels_dict[label_1] \n",
    "\n",
    "            # Select the best model\n",
    "            results = []\n",
    "            models = {}\n",
    "            \n",
    "            for model_name, model_info in model_storage.items():\n",
    "                # Directory to save project analysis\n",
    "                model_save_path = f'{analysis_path2}/pred_{label_0_name}_vs_{label_1_name}/{model_name}'\n",
    "\n",
    "                save_path = model_save_path + f'/metrics.json'\n",
    "                if os.path.exists(save_path):\n",
    "                    metrics_dict = load_json(save_path)\n",
    "                    results.append(metrics_dict)\n",
    "                    continue\n",
    "                    \n",
    "                # Pipeline with feature scaling and model\n",
    "                pipeline = ImbPipeline([\n",
    "                    ('scaler', MinMaxScaler()),  # Standardization\n",
    "                    ('oversample', SMOTE(random_state=42)),  # Oversample the minority class\n",
    "                    ('model', model_info['model']) # Model\n",
    "                ])\n",
    "                \n",
    "                # Perform Grid Search with Cross-Validation\n",
    "                grid_search = GridSearchCV(\n",
    "                    estimator=pipeline,\n",
    "                    param_grid=model_info['hyperparams'],\n",
    "                    cv=kf,\n",
    "                    scoring=balanced_accuracy_scorer,  # Options 'f1', 'roc_auc', 'precision', 'recall', 'balanced_accuracy' \n",
    "                                                       # f1 we care more about + detection, balanced_acc care about + and - class detection\n",
    "                    n_jobs=-1,\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                start_time = datetime.now()\n",
    "                print(f\"Starting Grid Search for {data_type} {time_frame} {model_name} {label_0_name}_vs_{label_1_name}. {start_time.strftime('%H:%M')}\")\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                print(\"Total runtime:\", (datetime.now() - start_time), \"HH:MM:SS\")\n",
    "\n",
    "                # Best parameters and model\n",
    "                best_pipeline = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "                print(f\"Best parameters for {model_name} {label_0_name}_vs_{label_1_name}: {best_params}\")\n",
    "                \n",
    "                # Use cross_val_predict for evaluation\n",
    "                y_pred = cross_val_predict(best_pipeline, X_train, y_train, cv=kf, method='predict')\n",
    "                y_pred_prob = cross_val_predict(best_pipeline, X_train, y_train, cv=kf, method='predict_proba')[:, 1]\n",
    "                \n",
    "                run_info = {\n",
    "                    'data_type': data_type,\n",
    "                    'time_frame': time_frame,\n",
    "                    'label_0': label_0_name,\n",
    "                    'label_1': label_1_name,\n",
    "                    'inputs': input_variable,\n",
    "                    'model_name': model_name,\n",
    "                    'best_params': best_params,\n",
    "                    'set': 'validation',\n",
    "                }\n",
    "\n",
    "                metrics = eval_report(y_train, y_pred, y_pred_prob, model_save_path, run_info)\n",
    "                results.append(metrics)\n",
    "                models[model_name] = best_pipeline\n",
    "\n",
    "            # Test set performance\n",
    "            test_save_path = f'{analysis_path2}/pred_{label_0_name}_vs_{label_1_name}/best_model_test_set'\n",
    "            if os.path.exists(test_save_path):\n",
    "                continue\n",
    "            if models == {}:\n",
    "                continue\n",
    "            \n",
    "            # Select rows with the highest AUC for each Output value\n",
    "            results_df = pd.DataFrame(results)\n",
    "            best_models_df = results_df.loc[results_df.groupby('labels')['balanced_accuracy_adjusted_chance'].idxmax()].reset_index()\n",
    "            if len(best_models_df)>1:\n",
    "                raise('too many matches')\n",
    "                \n",
    "            model_name = best_models_df['model'][0]\n",
    "            if model_name not in models.keys():\n",
    "                continue\n",
    "            best_pipeline = models[model_name]\n",
    "            \n",
    "            for X_test, y_test, names in BinaryLabelLoader.process_data_stream(df_test, [(label_0, label_1)]):\n",
    "                X_test = np.array(X_test)\n",
    "                y_test = np.array(y_test)\n",
    "        \n",
    "                # Use cross_val_predict for evaluation\n",
    "                y_pred = best_pipeline.predict(X_test)\n",
    "                y_pred_prob = best_pipeline.predict_proba(X_test)[:, 1]\n",
    "                \n",
    "                run_info = {\n",
    "                    'data_type': data_type,\n",
    "                    'time_frame': time_frame,\n",
    "                    'label_0': label_0_name,\n",
    "                    'label_1': label_1_name,\n",
    "                    'inputs': input_variable,\n",
    "                    'model_name': model_name,\n",
    "                    'best_params': best_params,\n",
    "                    'set': 'test',\n",
    "                }\n",
    "                \n",
    "                metrics = eval_report(y_test, y_pred, y_pred_prob, test_save_path, run_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set Metrics Summary\n",
    "Create a csv file that compiles the validation performance metrics from each finetuned model architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODEL SUMMARY (VALIDATION SET) ---\n",
    "results = []\n",
    "\n",
    "for time_frame in time_frame_list:\n",
    "    for data_type in data_type_list:\n",
    "        analysis_path = save_path_reference.replace('MODEL','classification_1class_meds').replace('INDEPENDENT_VAR',f'v5/{data_type}/{time_frame}') \n",
    "\n",
    "        # Load, select, and visualize data\n",
    "        _, df_data, continuous =  load_data(master_data_path, master_encoded_data_path, analysis_path, data_type, time_frame)\n",
    "        \n",
    "        # Prepare data for training\n",
    "        df_dev, df_test = split_data(df_data, y_col)\n",
    "\n",
    "        categorical = [col for col in df_data.columns if col not in continuous]\n",
    "        BinaryLabelLoader = MultiClassToBinaryConverter(y_col, categorical_cols=categorical)\n",
    "        \n",
    "        # Iterate over each label pair and predictor variable\n",
    "        for X_train, y_train, names in BinaryLabelLoader.process_data_stream(df_dev, paired_labels):\n",
    "            X_train = np.array(X_train)\n",
    "            y_train = np.array(y_train)\n",
    "            label1, label2, input_variable = names.values()\n",
    "            label_name1 = 'rest' if label1 == 'rest' else labels_dict[label1]\n",
    "            label_name2 = labels_dict[label2] \n",
    "\n",
    "            for model_name, model_info in model_storage.items():\n",
    "                # Directory to save project analysis\n",
    "                model_save_path = f'{analysis_path}/pred_{label_name1}_vs_{label_name2}/{model_name}'\n",
    "                #print(model_save_path)\n",
    "\n",
    "                save_path = model_save_path + '/metrics.json'\n",
    "                metrics_dict = load_json(save_path)\n",
    "                results.append(metrics_dict)\n",
    "    \n",
    "# Convert results to a DataFrame for better visualization\n",
    "print(\"Hyperparameter Tuning Results:\")\n",
    "results_df = pd.DataFrame(results)\n",
    "print(len(results_df))\n",
    "#print(results_df)\n",
    "\n",
    "# Save tree results summary\n",
    "comparison_dir = save_path_reference.replace('MODEL','classification_1class_meds').replace('INDEPENDENT_VAR',f'v5')\n",
    "csv_path = f'{comparison_dir}/tree_models_validation_summary.csv'\n",
    "print('run summary: ', csv_path)\n",
    "results_df.to_csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set Performance Metrics\n",
    "For each best finetuned model, calculate the test set performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TEST SET ---\n",
    "def map_label_name_to_num(labels_dict, label_name):\n",
    "    return [num for num, name in labels_dict.items() if name == label_name][0]\n",
    "\n",
    "for time_frame in time_frame_list:\n",
    "    for data_type in data_type_list:\n",
    "        analysis_path = save_path_reference.replace('MODEL','classification_1class_meds').replace('INDEPENDENT_VAR',f'v3/{data_type}/{time_frame}') \n",
    "        os.makedirs(analysis_path, exist_ok=True)\n",
    "\n",
    "        # Load, select, and visualize data\n",
    "        _, df_data, continuous =  load_data(master_data_path, master_encoded_data_path, analysis_path, data_type, time_frame)\n",
    "\n",
    "        # Prepare data for training\n",
    "        df_dev, df_test = split_data(df_data, y_col)\n",
    "\n",
    "        categorical = [col for col in df_data.columns if col not in continuous]\n",
    "        BinaryLabelLoader = MultiClassToBinaryConverter(y_col, categorical_cols=categorical)\n",
    "\n",
    "        analysis_path2 = analysis_path \n",
    "        os.makedirs(analysis_path2, exist_ok=True)\n",
    "        # Create a scorer object using balanced_accuracy_score\n",
    "        balanced_accuracy_scorer = make_scorer(balanced_accuracy_score, adjusted=True)\n",
    "\n",
    "        # Iterate over each label pair and predictor variable\n",
    "        for X_train, y_train, names in BinaryLabelLoader.process_data_stream(df_dev, paired_labels):\n",
    "            X_train = np.array(X_train)\n",
    "            y_train = np.array(y_train)\n",
    "            label_0, label_1, input_variable = names.values()\n",
    "            label_0_name = 'rest' if label_0 == 'rest' else labels_dict[label_0]\n",
    "            label_1_name = labels_dict[label_1] \n",
    "\n",
    "            # Select the best model\n",
    "            results = []\n",
    "\n",
    "            for model_name, model_info in model_storage.items():\n",
    "                # Directory to save project analysis\n",
    "                model_save_path = f'{analysis_path2}/pred_{label_0_name}_vs_{label_1_name}/{model_name}'\n",
    "                #print(model_save_path)\n",
    "\n",
    "                save_path = model_save_path + '/metrics.json'\n",
    "                metrics_dict = load_json(save_path)\n",
    "                results.append(metrics_dict)\n",
    "        \n",
    "            # Convert results to a DataFrame for better visualization\n",
    "            print(\"Hyperparameter Tuning Results:\")\n",
    "            results_df = pd.DataFrame(results)\n",
    "            #print(results_df)\n",
    "\n",
    "            # Save tree results summary\n",
    "            csv_path = f'{analysis_path2}/pred_{label_0_name}_vs_{label_1_name}/tree_models_summary.csv'\n",
    "            print('run summary: ', csv_path)\n",
    "            results_df.to_csv(csv_path)\n",
    "\n",
    "            # Select rows with the highest AUC for each Output value\n",
    "            best_models_df = results_df.loc[results_df.groupby('labels')['balanced_accuracy_adjusted_chance'].idxmax()].sort_values('labels').reset_index()\n",
    "            #print(best_models_df)\n",
    "\n",
    "            for index, row in best_models_df.iterrows():\n",
    "                model_name = row['model']\n",
    "                # DEBUG\n",
    "                if \"XGBoost\" in model_name:\n",
    "                    continue\n",
    "                hyperparams = row['best_parameters']\n",
    "                hyperparams = {key:[value] for key, value in hyperparams.items()}\n",
    "                label_0_name = row['label_0']\n",
    "                label_1_name = row['label_1']\n",
    "                model_info = model_storage[model_name]\n",
    "\n",
    "                label_0 = 'rest' if label_0_name == 'rest' else map_label_name_to_num(labels_dict, label_0_name)\n",
    "                label_1 = map_label_name_to_num(labels_dict, label_1_name)\n",
    "                \n",
    "                # Directory to save project analysis\n",
    "                model_save_path = f'{analysis_path2}/pred_{label_0_name}_vs_{label_1_name}/best_model_test_set'\n",
    "\n",
    "                if os.path.exists(f'{model_save_path}/metrics.json'):\n",
    "                    metrics_dict = load_json(f'{model_save_path}/metrics.json')\n",
    "                    #if metrics_dict['model'] == model_name:\n",
    "                    #    continue\n",
    "                if os.path.exists(f'{model_save_path}/SHAP_test/SHAP_heatmap.png'):\n",
    "                    continue\n",
    "\n",
    "                for X_train, y_train, names in BinaryLabelLoader.process_data_stream(df_dev, [(label_0, label_1)]):\n",
    "                    # SHAP Variables\n",
    "                    X_SHAP_train = X_train.copy(deep=True)\n",
    "\n",
    "                    # Training Variables\n",
    "                    X_train = np.array(X_train)\n",
    "                    y_train = np.array(y_train)\n",
    "\n",
    "                    # Pipeline with feature scaling and model\n",
    "                    pipeline = ImbPipeline([\n",
    "                        ('scaler', MinMaxScaler()),  # Standardization\n",
    "                        ('oversample', SMOTE(random_state=42)),  # Oversample the minority class\n",
    "                        ('model', model_info['model']) # Model\n",
    "                    ])\n",
    "                    \n",
    "                    # Perform Grid Search with Cross-Validation\n",
    "                    grid_search = GridSearchCV(\n",
    "                        estimator=pipeline,\n",
    "                        param_grid=hyperparams,\n",
    "                        scoring=balanced_accuracy_scorer, \n",
    "                        n_jobs=-1,\n",
    "                        verbose=1\n",
    "                    )\n",
    "                    \n",
    "                    print(f\"Starting Grid Search for {data_type} {time_frame} {model_name} {label_0_name}_vs_{label_1_name}...\")\n",
    "                    grid_search.fit(X_train, y_train)\n",
    "\n",
    "                    # Best parameters and model\n",
    "                    best_pipeline = grid_search.best_estimator_\n",
    "                    best_params = grid_search.best_params_\n",
    "                    print(f\"Best parameters for {model_name} {label_0_name}_vs_{label_1_name}: {best_params}\")\n",
    "\n",
    "                    # # --- SHAP Explainer ---\n",
    "                    # shap_analysis_pipeline(model = best_pipeline['model'], \n",
    "                    #                        X_SHAP = X_SHAP_train, \n",
    "                    #                        y_SHAP = np.array(y_train, dtype=bool), \n",
    "                    #                        model_savepath = f'{model_save_path}/SHAP_train_val')\n",
    "                    \n",
    "\n",
    "                for X_test, y_test, names in BinaryLabelLoader.process_data_stream(df_test, [(label_0, label_1)]):\n",
    "                    # SHAP Variables\n",
    "                    X_SHAP_test = X_test.copy(deep=True)\n",
    "\n",
    "                    X_test = np.array(X_test)\n",
    "                    y_test = np.array(y_test)\n",
    "\n",
    "                    # --- Performance Metrics ---\n",
    "                    # Use cross_val_predict for evaluation\n",
    "                    y_pred = best_pipeline.predict(X_test)\n",
    "                    y_pred_prob = best_pipeline.predict_proba(X_test)[:, 1]\n",
    "                    \n",
    "                    run_info = {\n",
    "                        'data_type': data_type,\n",
    "                        'time_frame': time_frame,\n",
    "                        'label_0': label_0_name,\n",
    "                        'label_1': label_1_name,\n",
    "                        'inputs': input_variable,\n",
    "                        'model_name': model_name,\n",
    "                        'best_params': best_params,\n",
    "                        'set': 'test',\n",
    "                    }\n",
    "\n",
    "                    metrics = eval_report(y_test, y_pred, y_pred_prob, model_save_path, run_info)\n",
    "\n",
    "                    # --- SHAP Explainer ---\n",
    "                    shap_analysis_pipeline(model = best_pipeline['model'], \n",
    "                                           X_SHAP = X_SHAP_test, \n",
    "                                           y_SHAP = np.array(y_test, dtype=bool), \n",
    "                                           model_savepath = f'{model_save_path}/SHAP_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set Metrics Summary\n",
    "Create a csv file that compiles the test set performance metrics from each best finetuned model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODEL SUMMARY (TEST SET) ---\n",
    "results = []\n",
    "\n",
    "for time_frame in time_frame_list:\n",
    "    for data_type in data_type_list:\n",
    "        analysis_path = save_path_reference.replace('MODEL','classification_1class_meds').replace('INDEPENDENT_VAR',f'v3/{data_type}/{time_frame}') \n",
    "\n",
    "        # Load, select, and visualize data\n",
    "        _, df_data, continuous =  load_data(master_data_path, master_encoded_data_path, analysis_path, data_type, time_frame)\n",
    "        \n",
    "        # Prepare data for training\n",
    "        df_dev, df_test = split_data(df_data, y_col)\n",
    "\n",
    "        categorical = [col for col in df_data.columns if col not in continuous]\n",
    "        BinaryLabelLoader = MultiClassToBinaryConverter(y_col, categorical_cols=categorical)\n",
    "        \n",
    "        # Iterate over each label pair and predictor variable\n",
    "        for X_train, y_train, names in BinaryLabelLoader.process_data_stream(df_dev, paired_labels):\n",
    "            X_train = np.array(X_train)\n",
    "            y_train = np.array(y_train)\n",
    "            label1, label2, input_variable = names.values()\n",
    "            label_name1 = 'rest' if label1 == 'rest' else labels_dict[label1]\n",
    "            label_name2 = labels_dict[label2] \n",
    "\n",
    "            # Directory to save project analysis\n",
    "            model_save_path = f'{analysis_path}/pred_{label_name1}_vs_{label_name2}/best_model_test_set'\n",
    "            #print(model_save_path)\n",
    "\n",
    "            save_path = model_save_path + '/metrics.json'\n",
    "            metrics_dict = load_json(save_path)\n",
    "            results.append(metrics_dict)\n",
    "    \n",
    "# Convert results to a DataFrame for better visualization\n",
    "print(\"Test Set Results:\")\n",
    "results_df = pd.DataFrame(results)\n",
    "print(len(results_df))\n",
    "#print(results_df)\n",
    "\n",
    "# Save tree results summary\n",
    "comparison_dir = save_path_reference.replace('MODEL','classification_1class_meds').replace('INDEPENDENT_VAR',f'v3')\n",
    "csv_path = f'{comparison_dir}/tree_models_test_summary_v3.csv'\n",
    "print('run summary: ', csv_path)\n",
    "results_df.to_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Feature Importance Rank Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SHAP FEATURE RANK AGGREGATION ---\n",
    "def rank_data_selection(time_frame_list, data_type_list, paired_labels):\n",
    "    csv_list = []\n",
    "    for time_frame in time_frame_list:\n",
    "        for data_type in data_type_list:\n",
    "            analysis_path = save_path_reference.replace('MODEL','classification_1class_meds').replace('INDEPENDENT_VAR',f'v3/{data_type}/{time_frame}') \n",
    "\n",
    "            # Iterate over each label pair and predictor variable\n",
    "            for label1, label2 in paired_labels:\n",
    "                label_name1 = 'rest' if label1 == 'rest' else labels_dict[label1]\n",
    "                label_name2 = labels_dict[label2] \n",
    "\n",
    "                # Directory to save project analysis\n",
    "                model_save_path = f'{analysis_path}/pred_{label_name1}_vs_{label_name2}/best_model_test_set'\n",
    "\n",
    "                csv_list.append(f\"{model_save_path}/SHAP_test/SHAP_mean_importance.csv\")\n",
    "    return csv_list\n",
    "\n",
    "def rank_best_data_selection(df, metric='balanced_accuracy_mean'):\n",
    "\n",
    "    # Find the indices of the rows with the highest balanced_accuracy for each label\n",
    "    idx = df.groupby('labels')[metric].idxmax()\n",
    "    # Use these indices to filter the DataFrame\n",
    "    df = df.loc[idx]\n",
    "\n",
    "    csv_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        time_frame = row['timeframe']\n",
    "        data_type = row['datatype']\n",
    "        analysis_path = save_path_reference.replace('MODEL','classification_1class_meds').replace('INDEPENDENT_VAR',f'v3/{data_type}/{time_frame}') \n",
    "        \n",
    "        label_name1 = row['label_0']\n",
    "        label_name2 = row['label_1']\n",
    "        # Directory to save project analysis\n",
    "        model_save_path = f'{analysis_path}/pred_{label_name1}_vs_{label_name2}/best_model_test_set'\n",
    "\n",
    "        csv_list.append(f\"{model_save_path}/SHAP_test/SHAP_mean_importance.csv\")\n",
    "    return csv_list\n",
    "\n",
    "def rank_prep(file_path, feature_col, metric_col):\n",
    "    \"\"\"Process individual CSV file and return normalized Borda scores\"\"\"\n",
    "    # Read CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Filter out rows with metric_col < 0.01\n",
    "    df_filtered = df[df[metric_col] >= 0.01]\n",
    "    \n",
    "    if df_filtered.empty:\n",
    "        return pd.Series(dtype=float)\n",
    "    \n",
    "    # Sort by metric_col descending\n",
    "    df_sorted = df_filtered.sort_values(metric_col, ascending=False)\n",
    "    \n",
    "    # Assign Borda points (highest metric gets highest points)\n",
    "    m = len(df_sorted)\n",
    "    df_sorted['borda'] = range(m, 0, -1)\n",
    "    \n",
    "    # Normalize scores to sum to 1\n",
    "    total_points = df_sorted['borda'].sum()\n",
    "    df_sorted['normalized'] = df_sorted['borda'] / total_points\n",
    "    \n",
    "    return df_sorted.set_index(feature_col)['normalized']\n",
    "\n",
    "def rank_aggregation(mean_shap_path_list):\n",
    "        # Process all files and collect scores\n",
    "    all_scores = []\n",
    "    all_features = set()\n",
    "\n",
    "    for file in mean_shap_path_list:\n",
    "        scores = rank_prep(file, feature_col='Feature', metric_col='Mean_SHAP')\n",
    "        all_scores.append(scores)\n",
    "        all_features.update(scores.index.tolist())\n",
    "\n",
    "    # Create a DataFrame with all cities\n",
    "    all_features = list(all_features)\n",
    "    borda_df = pd.DataFrame(index=all_features)\n",
    "\n",
    "    # Add each person's scores (0 for unranked cities)\n",
    "    for i, scores in enumerate(all_scores):\n",
    "        borda_df[f'model_{i+1}'] = scores.reindex(all_features).fillna(0)\n",
    "\n",
    "    # Calculate total Borda scores\n",
    "    borda_df['total_score'] = borda_df.sum(axis=1)\n",
    "    borda_df = borda_df.sort_values(by='total_score' ,ascending=False)\n",
    "    \n",
    "    # Sumarize findings\n",
    "    borda_df.index.name = 'feature'\n",
    "    borda_df = borda_df.reset_index()\n",
    "    borda_df['rank'] = np.arange(1,len(borda_df)+1)\n",
    "\n",
    "    # # Get final ranking\n",
    "    # final_ranking = borda_df['total_score'].sort_values(ascending=False)\n",
    "    #print(\"Final Borda Count Ranking:\")\n",
    "    #print(final_ranking.to_string(float_format='%.3f'))\n",
    "    \n",
    "    return borda_df\n",
    "\n",
    "# List of input CSV files (replace with your actual file paths)\n",
    "mean_shap_path_list = rank_best_data_selection(df=results_df[results_df['timeframe'] == \"2012_to_2024\"], metric='balanced_accuracy_mean')\n",
    "tab_text_ranked_df = rank_aggregation(mean_shap_path_list)\n",
    "print('best bala acc ranked feature importance')\n",
    "print(tab_text_ranked_df['feature'][0:10].to_string(float_format='%.3f'))\n",
    "\n",
    "mean_shap_path_list = rank_data_selection(time_frame_list=['2012_to_2024'], data_type_list=['tabular_text'], paired_labels=paired_labels)\n",
    "tab_text_ranked_df = rank_aggregation(mean_shap_path_list)\n",
    "print('tabular_text ranked feature importance')\n",
    "print(tab_text_ranked_df['feature'][0:10].to_string(float_format='%.3f'))\n",
    "\n",
    "mean_shap_path_list = rank_data_selection(time_frame_list=['2012_to_2024'], data_type_list=['tabular'], paired_labels=paired_labels)\n",
    "tab_ranked_df = rank_aggregation(mean_shap_path_list)\n",
    "print('tabular ranked feature importance')\n",
    "print(tab_ranked_df['feature'][0:10].to_string(float_format='%.3f'))\n",
    "\n",
    "mean_shap_path_list = rank_data_selection(time_frame_list=['2012_to_2024'], data_type_list=['text'], paired_labels=paired_labels)\n",
    "text_ranked_df = rank_aggregation(mean_shap_path_list)\n",
    "print('text ranked feature importance')\n",
    "print(text_ranked_df['feature'][0:10].to_string(float_format='%.3f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env_EMR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
