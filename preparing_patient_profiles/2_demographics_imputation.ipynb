{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f0b11f-860b-478f-9997-771ff56a4f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import pyrootutils\n",
    "root = pyrootutils.setup_root(\n",
    "    search_from=os.path.abspath(''),\n",
    "    indicator=[\".git\"],\n",
    "    pythonpath=True, # add root directory to the PYTHONPATH (helps with imports)\n",
    "    dotenv=True, # load environment variables from .env if exists in root directory\n",
    ")\n",
    "\n",
    "from utils.file_management.config_loader import load_yaml, process_config_values\n",
    "from utils.file_management.file_manager import FileManager\n",
    "from utils.query_utils.extractor import Extractor\n",
    "\n",
    "from tableone import TableOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e0b7d7-bb56-4ff5-999b-18cfda866010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load yaml file with dataset information\n",
    "cohort_cfg_path = str(root) + '/code/config/LBP_cohort.yaml'\n",
    "config = process_config_values(load_yaml(cohort_cfg_path))\n",
    "\n",
    "# Load paths to data\n",
    "PlumsFiles = FileManager(config.get('file_directory'))\n",
    "\n",
    "# Identify Patients\n",
    "patientdurablekey_list = pd.read_csv(PlumsFiles.get_datapath('patientdurablekey_csv'))\n",
    "patientdurablekey_list = list(patientdurablekey_list['patientdurablekey'])\n",
    "print(len(patientdurablekey_list))\n",
    "\n",
    "# Identify Imaging ID (accessions)\n",
    "accessionnumber_list = pd.read_csv(PlumsFiles.get_datapath('accessionnumber_csv'))\n",
    "accessionnumber_list = list(accessionnumber_list['accessionnumber'])\n",
    "print(len(accessionnumber_list))\n",
    "\n",
    "# Initialize data extraction tools\n",
    "check_query_flag = True       #axilluary checks to see if query makes sense\n",
    "PlumsExtractor = Extractor(num_results_flag=True, display_results_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62c7721-7152-4f28-86f1-560760d657d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Demographic Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4daf96-5298-43b7-a7c2-243e9a6f1fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictor Labels\n",
    "\n",
    "dataQuery = f'''\n",
    "/*\n",
    "Description: Patient demographics\n",
    "*/\n",
    "\n",
    "SELECT DISTINCT\n",
    "    patientdurablekey,\n",
    "    ageatfirstimaging,\n",
    "    yearatfirstimaging,\n",
    "    sex,\n",
    "    preferredlanguage,\n",
    "    raceethnicity,\n",
    "    smokingstatus,\n",
    "    religion,\n",
    "    socialsupport\n",
    "FROM \n",
    "  read_parquet('{PlumsFiles.get_datapath('patdurabledim_analysis_parquet')}')\n",
    "WHERE \n",
    "  patientdurablekey IN {tuple(patientdurablekey_list)}\n",
    "\n",
    "ORDER BY\n",
    "  patientdurablekey\n",
    "'''\n",
    "\n",
    "# Run query and update relevant keys\n",
    "results_df_pd = PlumsExtractor.run_query(dataQuery,runtime_flag=True,df_type='pandas')\n",
    "\n",
    "#Check whether query makes sense\n",
    "if check_query_flag==True:\n",
    "    PlumsExtractor.col_to_list(results_df_pd, 'patientdurablekey')\n",
    "    \n",
    "df_predictors1 = results_df_pd.copy()\n",
    "\n",
    "results_df_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cda37d-d263-44c7-9401-360dd64e441e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictor Labels\n",
    "dataQuery = f'''\n",
    "/*\n",
    "Description: Patient insurance\n",
    "*/\n",
    "\n",
    "SELECT DISTINCT\n",
    "    a.patientdurablekey,\n",
    "    b.primaryinsurance\n",
    "FROM\n",
    "(\n",
    "    SELECT DISTINCT\n",
    "        patientdurablekey,\n",
    "        MAX(primaryinsurancekey) AS primaryinsurancekey, \n",
    "    FROM \n",
    "      read_parquet('{PlumsFiles.get_datapath('billingaccountfact_analysis_parquet')}') \n",
    "    WHERE \n",
    "      patientdurablekey IN {tuple(patientdurablekey_list)}\n",
    "    GROUP BY\n",
    "      patientdurablekey\n",
    ") as a\n",
    "INNER JOIN \n",
    "    read_parquet('{PlumsFiles.get_datapath('billingaccountfact_analysis_parquet')}') as b\n",
    "ON a.primaryinsurancekey = b.primaryinsurancekey\n",
    "\n",
    "ORDER BY\n",
    "  a.patientdurablekey\n",
    "'''\n",
    "\n",
    "# Run query and update relevant keys\n",
    "results_df_pd = PlumsExtractor.run_query(dataQuery,runtime_flag=True,df_type='pandas')\n",
    "\n",
    "#Check whether query makes sense\n",
    "# if check_query_flag==True:\n",
    "#     PlumsExtractor.col_to_list(results_df_pd, 'patientdurablekey')\n",
    "\n",
    "df_predictors2 = results_df_pd.copy()\n",
    "\n",
    "results_df_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e0da6-0e25-40c7-9b7a-7b12036c61ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify patients without any interventions\n",
    "missing_patients = list(set(patientdurablekey_list) - set(df_predictors2['patientdurablekey']))\n",
    "print(len(missing_patients))\n",
    "# Create a new DataFrame with the new keys and label set to 0\n",
    "df_missing = pd.DataFrame({'patientdurablekey': missing_patients, 'primaryinsurance': 'unknown'})\n",
    "# Append the new rows to the existing DataFrame\n",
    "df_predictors2 = pd.concat([df_predictors2, df_missing], ignore_index=True)\n",
    "df_predictors2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8e0eb7-3203-46ae-a689-204b0c4f650c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictor Labels\n",
    "dataQuery = f'''\n",
    "/*\n",
    "Description: Patient diagnoses\n",
    "*/\n",
    "\n",
    "SELECT DISTINCT\n",
    "    *\n",
    "FROM \n",
    "  read_parquet('{PlumsFiles.get_datapath('diagnosiseventfact_analysis_parquet')}')\n",
    "WHERE \n",
    "  patientdurablekey IN {tuple(patientdurablekey_list)}\n",
    "        \n",
    "ORDER BY\n",
    "  patientdurablekey\n",
    "'''\n",
    "\n",
    "# Run query and update relevant keys\n",
    "results_df_pd = PlumsExtractor.run_query(dataQuery,runtime_flag=True,df_type='pandas')\n",
    "\n",
    "#Check whether query makes sense\n",
    "if check_query_flag==True:\n",
    "    PlumsExtractor.col_to_list(results_df_pd, 'patientdurablekey')\n",
    "\n",
    "df_predictors3 = results_df_pd.copy()\n",
    "\n",
    "# Set 'key' as index if needed\n",
    "df_predictors3 = results_df_pd.copy()\n",
    "df_predictors3['lbpduration'][df_predictors3['lbpduration']=='unspecified'] = 0\n",
    "df_predictors3['lbpduration'][df_predictors3['lbpduration']=='acute'] = 1\n",
    "df_predictors3['lbpduration'][df_predictors3['lbpduration']=='chronic'] = 2\n",
    "df_predictors3 = df_predictors3.groupby('patientdurablekey', as_index=True).max().reset_index()\n",
    "\n",
    "results_df_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b56bb6-cad2-4827-8c6b-8a9360bc01c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictor Labels\n",
    "dataQuery = f'''\n",
    "/*\n",
    "Description: Combine patient info into a patient profile\n",
    "*/\n",
    "\n",
    "SELECT DISTINCT\n",
    "    *\n",
    "FROM \n",
    "  (SELECT DISTINCT\n",
    "  a.*,\n",
    "  b.primaryinsurance\n",
    "  FROM df_predictors1 as a\n",
    "  INNER JOIN df_predictors2 as b\n",
    "    ON a.patientdurablekey = b.patientdurablekey\n",
    "  ) as d\n",
    "INNER JOIN df_predictors3 as c\n",
    "ON d.patientdurablekey = c.patientdurablekey\n",
    "        \n",
    "ORDER BY\n",
    "  c.patientdurablekey\n",
    "'''\n",
    "\n",
    "# Run query and update relevant keys\n",
    "results_df_pd = PlumsExtractor.run_query(dataQuery,runtime_flag=True,df_type='pandas')\n",
    "\n",
    "#Check whether query makes sense\n",
    "if check_query_flag==True:\n",
    "    PlumsExtractor.col_to_list(results_df_pd, 'patientdurablekey')\n",
    "\n",
    "df = results_df_pd.copy().drop(['patientdurablekey_1'], axis=1)\n",
    "df = df.drop(['anxiety','depression'],axis=1)\n",
    "results_df_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eb5b9f-9fc7-4d7d-8794-3ae995972878",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary = TableOne(df, \n",
    "                      categorical=['sex', 'preferredlanguage', 'raceethnicity', 'smokingstatus', 'religion',\n",
    "                                   'socialsupport', 'primaryinsurance', 'negativepsychstate', 'obesity',\n",
    "                                   'lbpduration', 'sciatica', 'facetjointarthropathy', 'scoliosis',\n",
    "                                   'discpathology', 'spinalstenosis', 'sacroiliacjoint', 'diabetes'],\n",
    "                      \n",
    "                      continuous=['ageatfirstimaging', 'yearatfirstimaging'])\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78dcfcc-d81e-43a4-99e7-8501a65c4b77",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Determine type of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7767c3b1-400a-4bea-bdc9-f1e047e3fd84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from missforest import MissForest\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f72342-b227-4e50-93f2-88705b3e2ce8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f40886-464f-48d3-98cd-74d409bdca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_vars = ['smokingstatus', 'primaryinsurance', 'socialsupport', 'raceethnicity'] #'religion', \n",
    "predictors = ['sex', 'ageatfirstimaging', 'yearatfirstimaging', 'preferredlanguage',\n",
    "             'smokingstatus', 'primaryinsurance', 'socialsupport', 'raceethnicity']\n",
    "# predictors = ['sex', 'ageatfirstimaging', 'yearatfirstimaging', 'preferredlanguage']\n",
    "# predictors = ['sex', 'ageatfirstimaging', 'yearatfirstimaging', 'preferredlanguage',\n",
    "#              'negativepsychstate', 'obesity', 'lbpduration', 'sciatica', 'facetjointarthropathy', \n",
    "#              'scoliosis', 'discpathology', 'spinalstenosis', 'sacroiliacjoint', 'diabetes'] ##'religion', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893e10b6-d334-4e68-8551-a3c28bd4e179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count the number of 'unknown' in each row\n",
    "unknown_counts = df.apply(lambda row: (row == 'unknown').sum(), axis=1)\n",
    "\n",
    "# Generate summary\n",
    "summary = Counter(unknown_counts)\n",
    "\n",
    "# Convert summary to a readable format\n",
    "summary_df = pd.DataFrame.from_dict(summary, orient='index', columns=['Row Count']).sort_index()\n",
    "summary_df.index.name = 'Number of Unknowns'\n",
    "\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10751f22-9847-4123-885d-c732fbf1bc3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to count other columns with 'unknown' when column 'a' is 'unknown'\n",
    "def count_unknowns_when_col_unknown(row, col):\n",
    "    if row[col] == 'unknown':\n",
    "        return (row.drop(col) == 'unknown').sum()  # Count 'unknown' in other columns\n",
    "    return 0  # Return 0 if column 'a' is not 'unknown'\n",
    "\n",
    "# Apply the function to each row\n",
    "for col in missing_vars:\n",
    "    total_other_unknowns = df.apply(lambda row: count_unknowns_when_col_unknown(row, col), axis=1)\n",
    "    print(f\"When column '{col}' is 'unknown', total number of other 'unknown' fields:\")\n",
    "    print(total_other_unknowns.value_counts())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a66dae-7547-425a-9462-c6a633ff102f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for var in missing_vars:\n",
    "    # Change unknown to nan variables to so that we can use dataframe functions to drop\n",
    "    df[f'{var}'] = [np.nan if x=='unknown' else x for x in df[var]]\n",
    "    # Create missingness indicator\n",
    "    df[f'{var}_missing'] = df[var].isna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbb9726-9cd6-47fc-a905-7adb1c0a3cfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30db4806-d769-4e81-b700-2d9b640cf42f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Little's MCAR test\n",
    "test the null hypothesis that data is MCAR. Significant results suggest data is not MCAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a373a72-4f85-4608-8d15-86f041ca86ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for var in missing_vars:\n",
    "    # Adjust predictors\n",
    "    adjusted_predictors = [val for val in predictors if val!=var]\n",
    "    categorical_predictors = [val for val in adjusted_predictors if val!='ageatfirstimaging' and val!='yearatfirstimaging']\n",
    "    \n",
    "    # Drop rows where the 'adjusted_predictors' columns have missing values\n",
    "    df2 = df.dropna(subset=adjusted_predictors)  # Drop rows with missing predictors\n",
    "    \n",
    "    df_summary = TableOne(df2, \n",
    "                          groupby=f'{var}_missing', \n",
    "                          categorical=categorical_predictors,\n",
    "                          continuous=['ageatfirstimaging', 'yearatfirstimaging'],\n",
    "                          pval=True)\n",
    "    print(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08b6a64-743a-427a-8365-341ed6ba5257",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Logistic regression for each variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d648bd7-d659-42e7-95cd-3fc25f3d93fe",
   "metadata": {},
   "source": [
    "Create binary indicators (missing = 1, non-missing = 0) for each variable.\n",
    "Run logistic regression with observed variables as predictors. If missingness is significantly associated with observed variables, the data is likely MAR.\n",
    "\n",
    "Hypotheses\n",
    "* Patients who are smokers may refuse to answer (MNAR).\n",
    "* Missingness may depend on observed variables like age or language (MAR).\n",
    "* Patients refusing to disclose religion may be systematic (MNAR).\n",
    "* Missingness could relate to observed variables like sex or age (MAR).\n",
    "* Non-response might depend on language or demographic reluctance (MAR).\n",
    "\n",
    "Interpretation\n",
    "Logistic Regression Coefficients\n",
    "* Coefficiens: Indicates the direction and strength of the relationship between a predictor and the outcome (missingness).\n",
    "    * Positive (+): Increases the likelihood of the event (missing data).\n",
    "    * Negative (−): Decreases the likelihood of the event (missing data).\n",
    "* P-value: Shows the statistical significance of the predictor.\n",
    "    * 𝑝<0.05: Statistically significant.\n",
    "    * 𝑝≥0.05: Not statistically significant.\n",
    "* Meaning:\n",
    "    * +β, p<0.05 = The predictor increases the likelihood of missingness.\n",
    "    * -β, p<0.05 = The predictor decreases the likelihood of missingness.\n",
    "    * +β, p>0.05 = The predictor has a weak or no association with missingness (trend not significant).\n",
    "    * -β, p>0.05 = The predictor has a weak or no association with missingness (trend not significant).\n",
    "    * B~=0 = The predictor has almost no impact on missingness.\n",
    "    \n",
    "Metrics like Accuracy, Precision, and Recall from the classification_report give insight into how well the model predicts missingness.\n",
    "* Good model performance: Supports MAR, as the missingness is explainable by observed data.\n",
    "* Poor model performance: Suggests missingness may be completely random (MCAR) or dependent on unobserved factors (MNAR).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707f4e3d-f15b-4069-a307-7b77bef80564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Variation inflation factor (VIF)\n",
    "\n",
    "\"\"\"\n",
    "Variance Inflation Factor (VIF) is a metric used to detect multicollinearity in a regression model. Multicollinearity occurs \n",
    "when predictor variables are highly correlated, which can make it difficult to estimate the effect of each predictor accurately\n",
    "due to inflate standard errors of coefficients, reduced statistical power, and unreliability regression estimates.\n",
    "\n",
    "Interpreting VIF:\n",
    "* VIF=1: No correlation with other variables.\n",
    "* 1<VIF<5: Moderate correlation; generally acceptable.\n",
    "* VIF≥5: High multicollinearity; consider addressing it.\n",
    "* VIF≥10: Severe multicollinearity; problematic.\n",
    "\"\"\"\n",
    "\n",
    "for var in missing_vars:\n",
    "    # Adjust predictors\n",
    "    adjusted_predictors = [val for val in predictors if val!=var]\n",
    "    categorical_predictors = [val for val in adjusted_predictors if val!='ageatfirstimaging' and val!='yearatfirstimaging']\n",
    "    \n",
    "    # Drop rows where the 'adjusted_predictors' columns have missing values\n",
    "    df2 = df.dropna(subset=adjusted_predictors)  # Drop rows with missing predictors\n",
    "    \n",
    "    y = df2[f'{var}_missing'].to_numpy()\n",
    "    X = df2[adjusted_predictors]\n",
    "    \n",
    "    # Encode the input variable and outcome variable as numbers\n",
    "    for cat_var in categorical_predictors:\n",
    "        X[cat_var] = label_encoder.fit_transform(X[cat_var].astype('str'))\n",
    "    \n",
    "    # Normalize each column/variable\n",
    "    X = (X-X.min())/(X.max()-X.min())\n",
    "    \n",
    "    # Add a constant for the intercept\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    # Calculate VIF for each feature\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Variable\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "    print(vif_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df4c687-f43c-4735-acac-3a7c7d2281ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PREDICTING MISSINGNESS WITH LOGISTIC REGRESSION\n",
    "for var in missing_vars:\n",
    "    # Adjust predictors\n",
    "    adjusted_predictors = [val for val in predictors if val!=var]\n",
    "    categorical_predictors = [val for val in adjusted_predictors if val!='ageatfirstimaging' and val!='yearatfirstimaging']\n",
    "    \n",
    "    # Drop rows where the 'adjusted_predictors' columns have missing values\n",
    "    df2 = df.dropna(subset=adjusted_predictors)  # Drop rows with missing predictors\n",
    "    \n",
    "    y = df2[f'{var}_missing'].to_numpy()\n",
    "    X = df2[adjusted_predictors]\n",
    "    \n",
    "    # Encode the input variable and outcome variable as numbers\n",
    "    label_encoders = {}\n",
    "    for cat_var in categorical_predictors:\n",
    "        label_encoders[cat_var] = LabelEncoder()\n",
    "        X[cat_var] = label_encoders.fit_transform(X[cat_var].astype('str'))\n",
    "    \n",
    "    # Normalize each column/variable\n",
    "    X = (X-X.min())/(X.max()-X.min())\n",
    "    \n",
    "    # Add a constant for the intercept\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Fit the logistic regression model\n",
    "    logit_model = sm.Logit(y_train, X_train)\n",
    "    result = logit_model.fit()\n",
    "\n",
    "    # Extract coefficients and p-values\n",
    "    summary_table = pd.DataFrame({\n",
    "        \"Predictor\": X.columns,\n",
    "        \"Coefficient\": result.params.values,\n",
    "        \"P-value\": result.pvalues.values\n",
    "    })\n",
    "\n",
    "    # Format table for better readability\n",
    "    summary_table['P-value'] = summary_table['P-value'].apply(lambda p: f\"{p:.3f}\")\n",
    "    summary_table['Coefficient'] = summary_table['Coefficient'].apply(lambda c: f\"{c:.3f}\")\n",
    "    \n",
    "    print(f\"Missingness Analysis for {var}:\")\n",
    "    print(summary_table)\n",
    "\n",
    "    # Predict probabilities on the test set\n",
    "    y_pred_prob = result.predict(X_test)\n",
    "    # Convert probabilities to binary predictions using a threshold (default = 0.5)\n",
    "    y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred, adjusted=False):.2f}\")\n",
    "    print(f\"Balanced Accuracy adjusted for chance: {balanced_accuracy_score(y_test, y_pred, adjusted=True):.2f}\")\n",
    "        \n",
    "    # Generate the confusion matrix\n",
    "    #cm = confusion_matrix(y_test, y_pred)\n",
    "    #print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ea268d-0711-44a0-bbd8-c13bd6705af2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PREDICTING MISSINGNESS WITH TREES\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "    \n",
    "for var in missing_vars:\n",
    "    # Adjust predictors\n",
    "    adjusted_predictors = [val for val in predictors if val!=var]\n",
    "    categorical_predictors = [val for val in adjusted_predictors if val!='ageatfirstimaging' and val!='yearatfirstimaging']\n",
    "    \n",
    "    # Drop rows where the 'adjusted_predictors' columns have missing values\n",
    "    df2 = df.dropna(subset=adjusted_predictors)  # Drop rows with missing predictors\n",
    "    \n",
    "    y = df2[f'{var}_missing'].to_numpy()\n",
    "    X = df2[adjusted_predictors]\n",
    "    \n",
    "    # Encode the input variable and outcome variable as numbers\n",
    "    label_encoder = LabelEncoder()\n",
    "    for cat_var in categorical_predictors:\n",
    "        X[cat_var] = label_encoder.fit_transform(X[cat_var].astype('str'))\n",
    "\n",
    "    # Normalize each column/variable\n",
    "    X = (X-X.min())/(X.max()-X.min())\n",
    "    \n",
    "    # Add a constant for the intercept\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train and evaluate tree-based models\n",
    "    models = {\n",
    "        \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "        \"Random Forest\": RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "        \"XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "    }\n",
    "    \n",
    "    print(f\"Missingness Analysis for {var}:\\n\")\n",
    "    for model_name, model in models.items():\n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Print performance metrics\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n",
    "        print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred, adjusted=False):.2f}\")\n",
    "        print(f\"Balanced Accuracy adjusted for chance: {balanced_accuracy_score(y_test, y_pred, adjusted=True):.2f}\")\n",
    "        # Generate the confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18d181b-89ac-4454-84d2-a2b1b89664aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e32ee2b-f912-44f3-9a0c-5f39d695a67b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imputation Experimentation with Artifical Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3bb044-4e45-46a1-8a98-5d559c45178a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the function to calculate accuracy for categorical variables\n",
    "def calculate_accuracy(true_values, predicted_values, missing_values):\n",
    "    \"\"\"\n",
    "    Calculate accuracy only for originally missing values.\n",
    "    \n",
    "    Parameters:\n",
    "        true_values (array-like): Ground truth values.\n",
    "        predicted_values (array-like): Imputed values.\n",
    "        missing_values (array-like): Indicator of missing values in the original data.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy for originally missing values.\n",
    "    \"\"\"\n",
    "    mask = pd.isna(missing_values)  # Exclude nan values from the calculation\n",
    "    true_values = true_values[mask]\n",
    "    predicted_values = predicted_values[mask]\n",
    "    return [accuracy_score(true_values, predicted_values), balanced_accuracy_score(true_values, predicted_values, adjusted=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409d7d44-a9ce-43b1-8422-b153adf0491a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define predictors\n",
    "predictors = ['sex', 'ageatfirstimaging', 'yearatfirstimaging', 'preferredlanguage',\n",
    "              'smokingstatus', 'primaryinsurance', 'socialsupport', 'raceethnicity',\n",
    "             'negativepsychstate', 'obesity', 'lbpduration', 'sciatica', 'facetjointarthropathy', \n",
    "             'scoliosis', 'discpathology', 'spinalstenosis', 'sacroiliacjoint', 'diabetes'] \n",
    "predictors = ['sex', 'ageatfirstimaging', 'yearatfirstimaging', 'preferredlanguage',\n",
    "             'smokingstatus', 'primaryinsurance', 'socialsupport', 'raceethnicity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbfb9ec",
   "metadata": {},
   "source": [
    "### MissForest Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93930a75-29d1-4c15-924c-ed108a2cda6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop rows where the 'adjusted_predictors' columns have missing values\n",
    "df2 = df.dropna(subset=predictors)  # Drop rows with missing predictors\n",
    "\n",
    "# Predictor type\n",
    "categorical_predictors = [val for val in predictors if val!='ageatfirstimaging' and val!='yearatfirstimaging']\n",
    "\n",
    "# Create a copy for the data to introduce artificial missingness\n",
    "X = df2[list(set(predictors+missing_vars))]\n",
    "X_missing = X.copy()\n",
    "\n",
    "# Create missing values.\n",
    "for var in missing_vars:    \n",
    "    n = int(len(X_missing) * 0.1)\n",
    "    rand_idx = np.random.choice(X_missing.index, n)\n",
    "    X_missing.loc[rand_idx, var] = np.nan\n",
    "\n",
    "# Split dataset into train and test sets.\n",
    "train_gt, test_gt = train_test_split(X, test_size=.3, shuffle=True,\n",
    "                               random_state=42)\n",
    "train_missing, test_missing = train_test_split(X_missing, test_size=.3, shuffle=True,\n",
    "                               random_state=42)\n",
    "#train_missing.drop(categorical_predictors, axis=1)\n",
    "\n",
    "# Default estimators are lgbm classifier and regressor (gradient boosting framework based on decision tree algorithms)\n",
    "mf = MissForest()\n",
    "mf.fit_transform(\n",
    "    x=train_missing,\n",
    "    categorical=categorical_predictors\n",
    ")\n",
    "train_imputed = mf.transform(x=train_missing)\n",
    "test_imputed = mf.transform(x=test_missing)\n",
    "\n",
    "# Calculate accuracy for categorical variables\n",
    "for var in missing_vars:\n",
    "    acc = calculate_accuracy(train_gt[var].values, train_imputed[var].values, train_missing[var].values)\n",
    "    print(f\"Train Accuracy for {var}: {acc[0]:.2f}, {acc[1]:.2f}\")\n",
    "    acc = calculate_accuracy(test_gt[var].values, test_imputed[var].values, test_missing[var].values)\n",
    "    print(f\"Test Accuracy for {var}: {acc[0]:.2f}, {acc[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643b27fe-b276-4ee7-9b6a-2d286a94e525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compare distributions of original vs. imputed data\n",
    "for idx, var in enumerate(missing_vars):\n",
    "    # Plot distributions of training data\n",
    "    plt.subplot(2,4,idx+1)\n",
    "    label_encoder = LabelEncoder()\n",
    "    x_missing = label_encoder.fit_transform(train_missing[var].dropna().astype('str'))\n",
    "    x_gt      = label_encoder.fit_transform(train_gt[var].astype('str'))\n",
    "    x_imputed = label_encoder.fit_transform(train_imputed[var].astype('str'))\n",
    "    sns.kdeplot(x_missing, label=\"Training Data\", color='green')\n",
    "    #sns.kdeplot(x_gt, label=\"Ground Truth Data\", color='blue')\n",
    "    sns.kdeplot(x_imputed, label=\"Imputed Data\", color='orange')\n",
    "    \n",
    "    # Set xticks for each integer\n",
    "    plt.xticks(np.arange(x_gt.min(), x_gt.max()+1, 1))\n",
    "    \n",
    "    # Plot distributions of testing data\n",
    "    plt.subplot(2,4,idx+5)\n",
    "    x_missing = label_encoder.fit_transform(test_missing[var].dropna().astype('str'))\n",
    "    x_gt      = label_encoder.fit_transform(test_gt[var].astype('str'))\n",
    "    x_imputed = label_encoder.fit_transform(test_imputed[var].astype('str'))\n",
    "    sns.kdeplot(x_missing, label=\"Training Data\", color='green')\n",
    "    #sns.kdeplot(x_gt, label=\"Ground Truth Data\", color='blue')\n",
    "    sns.kdeplot(x_imputed, label=\"Imputed Data\", color='orange')\n",
    "    \n",
    "    # Set xticks for each integer\n",
    "    plt.xticks(np.arange(x_gt.min(), x_gt.max()+1, 1))\n",
    "    plt.xlabel(var)\n",
    "    \n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4400c9",
   "metadata": {},
   "source": [
    "### KNNImputer Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb69b67-c594-409e-b804-6f2397b77cbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop rows where the 'adjusted_predictors' columns have missing values\n",
    "df2 = df.dropna(subset=predictors)  # Drop rows with missing predictors\n",
    "\n",
    "# Adjust predictors\n",
    "categorical_predictors = [val for val in predictors if val!='ageatfirstimaging' and val!='yearatfirstimaging']\n",
    "    \n",
    "# Encode the input variable and outcome variable as numbers\n",
    "label_encoder = LabelEncoder()\n",
    "for cat_var in categorical_predictors:\n",
    "    df2[cat_var] = label_encoder.fit_transform(df2[cat_var].astype('str'))\n",
    "    \n",
    "# Create a copy for the data to introduce artificial missingness\n",
    "X = df2[list(set(predictors+missing_vars))]\n",
    "X_missing = X.copy()\n",
    "\n",
    "# Create missing values.\n",
    "for var in missing_vars:    \n",
    "    n = int(len(X_missing) * 0.1)\n",
    "    rand_idx = np.random.choice(X_missing.index, n)\n",
    "    X_missing.loc[rand_idx, var] = np.nan\n",
    "\n",
    "# Split dataset into train and test sets.\n",
    "train_gt, test_gt = train_test_split(X, test_size=.3, shuffle=True,\n",
    "                               random_state=42)\n",
    "train_missing, test_missing = train_test_split(X_missing, test_size=.3, shuffle=True,\n",
    "                               random_state=42)\n",
    "\n",
    "# Default estimators are lgbm classifier and regressor (gradient boosting framework based on decision tree algorithms)\n",
    "mf = KNNImputer(n_neighbors=3)\n",
    "# mf.fit for mf.fit_transform\n",
    "mf.fit_transform(\n",
    "    X=train_missing\n",
    ")\n",
    "train_imputed = mf.transform(X=train_missing)\n",
    "train_imputed = pd.DataFrame(train_imputed, columns=X.columns)\n",
    "test_imputed = mf.transform(X=test_missing)\n",
    "test_imputed = pd.DataFrame(test_imputed, columns=X.columns)\n",
    "\n",
    "# Encode the input variable and outcome variable as numbers\n",
    "for cat_var in categorical_predictors:\n",
    "    train_imputed[cat_var] = train_imputed[cat_var].astype(int)\n",
    "    test_imputed[cat_var] = test_imputed[cat_var].astype(int)\n",
    "\n",
    "# Calculate accuracy for categorical variables\n",
    "for var in missing_vars:\n",
    "    acc = calculate_accuracy(train_gt[var].values, train_imputed[var].values, train_missing[var].values)\n",
    "    print(f\"Train Accuracy for {var}: {acc[0]:.2f}, {acc[1]:.2f}\")\n",
    "    acc = calculate_accuracy(test_gt[var].values, test_imputed[var].values, test_missing[var].values)\n",
    "    print(f\"Test Accuracy for {var}: {acc[0]:.2f}, {acc[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83f3c34-3036-4d28-9657-b6c87b065d1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compare distributions of original vs. imputed data\n",
    "for idx, var in enumerate(missing_vars):\n",
    "    # Plot distributions of training data\n",
    "    plt.subplot(2,4,idx+1)\n",
    "    label_encoder = LabelEncoder()\n",
    "    x_missing = label_encoder.fit_transform(train_missing[var].dropna().astype('str'))\n",
    "    x_gt      = label_encoder.fit_transform(train_gt[var].astype('str'))\n",
    "    x_imputed = label_encoder.fit_transform(train_imputed[var].astype('str'))\n",
    "    sns.kdeplot(x_missing, label=\"Training Data\", color='green')\n",
    "    #sns.kdeplot(x_gt, label=\"Ground Truth Data\", color='blue')\n",
    "    sns.kdeplot(x_imputed, label=\"Imputed Data\", color='orange')\n",
    "    \n",
    "    # Set xticks for each integer\n",
    "    plt.xticks(np.arange(x_gt.min(), x_gt.max()+1, 1))\n",
    "    \n",
    "    # Plot distributions of testing data\n",
    "    plt.subplot(2,4,idx+5)\n",
    "    x_missing = label_encoder.fit_transform(test_missing[var].dropna().astype('str'))\n",
    "    x_gt      = label_encoder.fit_transform(test_gt[var].astype('str'))\n",
    "    x_imputed = label_encoder.fit_transform(test_imputed[var].astype('str'))\n",
    "    sns.kdeplot(x_missing, label=\"Training Data\", color='green')\n",
    "    #sns.kdeplot(x_gt, label=\"Ground Truth Data\", color='blue')\n",
    "    sns.kdeplot(x_imputed, label=\"Imputed Data\", color='orange')\n",
    "    \n",
    "    # Set xticks for each integer\n",
    "    plt.xticks(np.arange(x_gt.min(), x_gt.max()+1, 1))\n",
    "    plt.xlabel(var)\n",
    "    \n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511ca928",
   "metadata": {},
   "source": [
    "### Iterative Imputer Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a73ad-8f44-4dba-94eb-2ce0a0dfa4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "# Drop rows where the 'adjusted_predictors' columns have missing values\n",
    "df2 = df.dropna(subset=predictors)  # Drop rows with missing predictors\n",
    "\n",
    "# Adjust predictors\n",
    "categorical_predictors = [val for val in predictors if val!='ageatfirstimaging' and val!='yearatfirstimaging']\n",
    "    \n",
    "# Encode the input variable and outcome variable as numbers\n",
    "label_encoder = LabelEncoder()\n",
    "for cat_var in categorical_predictors:\n",
    "    X[cat_var] = label_encoder.fit_transform(X[cat_var].astype('str'))\n",
    "\n",
    "# Create a copy for the data to introduce artificial missingness\n",
    "X = df2[list(set(predictors+missing_vars))]\n",
    "X_missing = X.copy()\n",
    "\n",
    "# Create missing values.\n",
    "for var in missing_vars:    \n",
    "    n = int(len(X_missing) * 0.1)\n",
    "    rand_idx = np.random.choice(X_missing.index, n)\n",
    "    X_missing.loc[rand_idx, var] = np.nan\n",
    "\n",
    "# Split dataset into train and test sets.\n",
    "train_gt, test_gt = train_test_split(X, test_size=.3, shuffle=True,\n",
    "                               random_state=42)\n",
    "train_missing, test_missing = train_test_split(X_missing, test_size=.3, shuffle=True,\n",
    "                               random_state=42)\n",
    "\n",
    "# \n",
    "mf = IterativeImputer(random_state=42)\n",
    "mf.fit_transform(\n",
    "    X=train_missing\n",
    ")\n",
    "train_imputed = mf.transform(X=train_missing)\n",
    "train_imputed = pd.DataFrame(train_imputed, columns=X.columns)\n",
    "test_imputed = mf.transform(X=test_missing)\n",
    "test_imputed = pd.DataFrame(test_imputed, columns=X.columns)\n",
    "\n",
    "# Encode the input variable and outcome variable as numbers\n",
    "for cat_var in categorical_predictors:\n",
    "    train_imputed[cat_var] = train_imputed[cat_var].astype(int)\n",
    "    test_imputed[cat_var] = test_imputed[cat_var].astype(int)\n",
    "\n",
    "# Calculate accuracy for categorical variables\n",
    "for var in missing_vars:\n",
    "    acc = calculate_accuracy(train_gt[var].values, train_imputed[var].values, train_missing[var].values)\n",
    "    print(f\"Train Accuracy for {var}: {acc[0]:.2f}, {acc[1]:.2f}\")\n",
    "    acc = calculate_accuracy(test_gt[var].values, test_imputed[var].values, test_missing[var].values)\n",
    "    print(f\"Test Accuracy for {var}: {acc[0]:.2f}, {acc[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924469c7",
   "metadata": {},
   "source": [
    "### Miceforest Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37777ad-ed9d-4ea7-8900-3c47eb557c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import miceforest as micef\n",
    "\n",
    "\n",
    "# Drop rows where the 'adjusted_predictors' columns have missing values\n",
    "df2 = df.dropna(subset=predictors)  # Drop rows with missing predictors\n",
    "\n",
    "# Adjust predictors\n",
    "categorical_predictors = [val for val in predictors if val!='ageatfirstimaging' and val!='yearatfirstimaging']\n",
    "    \n",
    "# Encode the input variable and outcome variable as numbers\n",
    "label_encoder = LabelEncoder()\n",
    "for cat_var in categorical_predictors:\n",
    "    df2[cat_var] = label_encoder.fit_transform(df2[cat_var].astype('str'))\n",
    "    \n",
    "# Create a copy for the data to introduce artificial missingness\n",
    "X = df2[list(set(predictors+missing_vars))]\n",
    "X_missing = X.copy()\n",
    "\n",
    "# Create missing values.\n",
    "for var in missing_vars:    \n",
    "    n = int(len(X_missing) * 0.1)\n",
    "    rand_idx = np.random.choice(X_missing.index, n)\n",
    "    X_missing.loc[rand_idx, var] = np.nan\n",
    "\n",
    "# Split dataset into train and test sets.\n",
    "train_gt, test_gt = train_test_split(X, test_size=.3, shuffle=True,\n",
    "                               random_state=42)\n",
    "train_missing, test_missing = train_test_split(X_missing, test_size=.3, shuffle=True,\n",
    "                               random_state=42)\n",
    "\n",
    "# Create ImputationKernel\n",
    "kernel = micef.ImputationKernel(\n",
    "    train_missing.reset_index(drop=True),  # Exclude the first column (School DBN)\n",
    "    save_all_iterations_data=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Perform MICE imputation\n",
    "kernel.mice(2)\n",
    "\n",
    "train_imputed = kernel.complete_data()\n",
    "train_imputed = pd.DataFrame(train_imputed, columns=X.columns)\n",
    "test_imputed = kernel.impute_new_data(test_missing.reset_index(drop=True))\n",
    "test_imputed = test_imputed.complete_data()\n",
    "\n",
    "# Encode the input variable and outcome variable as numbers\n",
    "for cat_var in categorical_predictors:\n",
    "    train_imputed[cat_var] = train_imputed[cat_var].astype(int)\n",
    "    test_imputed[cat_var] = test_imputed[cat_var].astype(int)\n",
    "\n",
    "# Calculate accuracy for categorical variables\n",
    "for var in missing_vars:\n",
    "    acc = calculate_accuracy(train_gt[var].values, train_imputed[var].values, train_missing[var].values)\n",
    "    print(f\"Train Accuracy for {var}: {acc[0]:.2f}, {acc[1]:.2f}\")\n",
    "    acc = calculate_accuracy(test_gt[var].values, test_imputed[var].values, test_missing[var].values)\n",
    "    print(f\"Test Accuracy for {var}: {acc[0]:.2f}, {acc[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb43f12-c077-43cf-bf7f-6a080f9dbe47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kernel.impute_new_data(test_missing.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0896c0d-c7cb-4a18-94d9-1052f50709f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Default estimators are lightgbm classifier and regressor (gradient boosting framework based on decision tree algorithms)\n",
    "# mf = MissForest()\n",
    "# mf.fit(\n",
    "#     x=X.dropna(subset=predictors),  # Drop rows with missing predictors,\n",
    "#     categorical=categorical_predictors\n",
    "# )\n",
    "# LGBM_imputed_df = mf.transform(x=X)\n",
    "\n",
    "\n",
    "\n",
    "# Mean absolute percentage error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bae11c-99aa-4b41-8895-917db05fefc4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imputation Exploration - Imputation Affect on Demographic distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c5d3fb-8352-42d2-85b2-9199a7d52d80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictors\n",
    "predictors = ['sex', 'ageatfirstimaging', 'yearatfirstimaging', 'preferredlanguage',\n",
    "             'smokingstatus', 'primaryinsurance', 'socialsupport', 'raceethnicity']\n",
    "\n",
    "# Adjust predictors\n",
    "categorical_predictors = [val for val in predictors if val!='ageatfirstimaging' and val!='yearatfirstimaging']\n",
    "    \n",
    "X = df[predictors].copy()\n",
    "    \n",
    "# Encode the input variable and preserve NaN\n",
    "label_encoder = LabelEncoder()\n",
    "for cat_var in categorical_predictors:\n",
    "    # Fit on non-missing values and transform while preserving NaN\n",
    "    non_nan_mask = ~X[cat_var].isna()\n",
    "    X.loc[non_nan_mask, cat_var] = label_encoder.fit_transform(X.loc[non_nan_mask, cat_var].astype('str'))\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1383433b-955c-4258-8063-b69ffb8dedaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96283932-1009-42f8-a39f-f88104240443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KNN Imputation\n",
    "'''\n",
    "This class fills in missing values using the k-Nearest Neighbors algorithm. It estimates the missing value by averaging \n",
    "the values of the k-nearest neighbors in the feature space.\n",
    "'''\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "imputed_data = imputer.fit_transform(X)\n",
    "imputed_df = pd.DataFrame(imputed_data, columns=X.columns)\n",
    "imputed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851cd7fa-48a4-45dc-b33b-02db333540e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Iterative Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a4cfc4-0897-48c5-bee9-54d37decb4c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterative Imputer\n",
    "'''\n",
    "This class models each feature with missing values as a function of other features, and uses iterative methods \n",
    "to estimate the missing values. It's more sophisticated than SimpleImputer and can be useful for complex datasets\n",
    "'''\n",
    "iter_imputer = IterativeImputer(random_state=42)\n",
    "imputed_data_iter = iter_imputer.fit_transform(X)\n",
    "imputed_df_iter = pd.DataFrame(imputed_data_iter, columns=X.columns)\n",
    "imputed_df_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c73f06c-5c65-45e9-8c6b-af03107d2fae",
   "metadata": {},
   "source": [
    "### MissForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc08b1c9-88f8-41e4-9953-d53dfd45c040",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create missing values.\n",
    "for c in X.columns:\n",
    "    n = int(len(df) * 0.1)\n",
    "    rand_idx = np.random.choice(X.index, n)\n",
    "    X.loc[rand_idx, c] = np.nan\n",
    "\n",
    "# Default estimators are lightgbm classifier and regressor (gradient boosting framework based on decision tree algorithms)\n",
    "mf = MissForest()\n",
    "mf.fit(\n",
    "    x=X.dropna(subset=predictors),  # Drop rows with missing predictors,\n",
    "    categorical=categorical_predictors\n",
    ")\n",
    "LGBM_imputed_df = mf.transform(x=X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d2085d-b4e2-4bc5-9703-6db9b95df443",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f62bba2-c9d2-4b3f-a967-a53b221c96e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compare distributions of original vs. imputed data\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "missing_vars = ['smokingstatus',\n",
    " 'primaryinsurance',\n",
    " 'socialsupport',\n",
    " 'raceethnicity']\n",
    "\n",
    "for idx, var in enumerate(missing_vars):\n",
    "    plt.subplot(2,3,idx+1)\n",
    "    sns.kdeplot(X[var].dropna(), label=\"Observed Data\", color='green')\n",
    "    sns.kdeplot(imputed_df[var], label=\"Imputed Data\", color='blue')\n",
    "    sns.kdeplot(imputed_df_iter[var], label=\"Iter Imputed Data\", color='orange')\n",
    "    sns.kdeplot(LGBM_imputed_df[var], label=\"Light GBM Imputed Data\", color='brown')\n",
    "    \n",
    "    # Set xticks for each integer\n",
    "    plt.xticks(np.arange(X[var].min(), X[var].max()+1, 1))\n",
    "    #plt.title(var)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0507ad3-7b61-450e-88ad-7dc4e1efe793",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Demographics Imputation for analysis\n",
    "KNNImpute as the best method to fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375ea470-87b0-40eb-9a38-625ee688d728",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adjust predictors\n",
    "categorical_predictors = [val for val in predictors if val!='ageatfirstimaging' and val!='yearatfirstimaging']\n",
    "\n",
    "X = df[predictors].copy()\n",
    "\n",
    "# Encode the input variable and preserve NaN\n",
    "label_encoders = {}\n",
    "for cat_var in categorical_predictors:\n",
    "    label_encoders[cat_var] = LabelEncoder()\n",
    "    # Fit on non-missing values and transform while preserving NaN\n",
    "    non_nan_mask = ~X[cat_var].isna()\n",
    "    X.loc[non_nan_mask, cat_var] = label_encoders[cat_var].fit_transform(X.loc[non_nan_mask, cat_var].astype('str'))\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f7165-7eeb-4622-b018-9100f3524ad0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KNN Imputation\n",
    "'''\n",
    "This class fills in missing values using the k-Nearest Neighbors algorithm. It estimates the missing value by averaging \n",
    "the values of the k-nearest neighbors in the feature space.\n",
    "'''\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "imputed_data = imputer.fit_transform(X)\n",
    "imputed_df = pd.DataFrame(imputed_data, columns=X.columns)\n",
    "imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2543dfff-b83e-4dfb-b2d0-fbda5990c203",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inverse transform encoded categorical variables\n",
    "imputed_X = X.copy()\n",
    "for cat_var in categorical_predictors:\n",
    "    # Decode only valid encoded values, leave NaN as is\n",
    "    imputed_X[cat_var] = imputed_df[cat_var].round().astype('int')  # Ensure int type for inverse transform\n",
    "    valid_values_mask = df[cat_var] != imputed_df[cat_var].isna()  # Detect missing values\n",
    "    imputed_X.loc[valid_values_mask, cat_var] = label_encoders[cat_var].inverse_transform(imputed_X.loc[valid_values_mask, cat_var].astype('int'))\n",
    "\n",
    "imputed_X['patientdurablekey'] = df['patientdurablekey']\n",
    "imputed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87295e9-241a-4994-86d9-f3b6894cd559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save imputed patdurabledim_analysis_parquet, and billingaccountfact_analysis_parquet\n",
    "PlumsFiles.save_df_to_parquet(imputed_X,PlumsFiles.get_datapath('patdurabledim_analysis_imputed_parquet'))\n",
    "PlumsFiles.save_df_to_csv(imputed_X,PlumsFiles.get_datapath('patdurabledim_analysis_imputed_csv'))\n",
    "\n",
    "#Check whether query makes sense\n",
    "if check_query_flag==True:\n",
    "    PlumsExtractor.col_to_list(imputed_X, 'patientdurablekey')\n",
    "\n",
    "imputed_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9370e19c-816c-43e9-99f1-bd3fa9389c4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env_emr",
   "language": "python",
   "name": "py_env_emr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
